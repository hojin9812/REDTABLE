{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from torchcrf import CRF\n",
    "import copy, json, os, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class SlotClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_slot_labels, dropout_rate=0.):\n",
    "        super(SlotClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_slot_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "\n",
    "    Args:\n",
    "        guid: Unique id for the example.\n",
    "        words: list. The words of the sequence.\n",
    "        intent_label: (Optional) string. The intent label of the example.\n",
    "        slot_labels: (Optional) list. The slot labels of the example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, intent_label=None, slot_labels=None):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.intent_label = intent_label\n",
    "        self.slot_labels = slot_labels\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, intent_label_id, slot_labels_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.intent_label_id = intent_label_id\n",
    "        self.slot_labels_ids = slot_labels_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_path = \"monologg/koelectra-base-v3-discriminator\"\n",
    "tokenizer = ElectraTokenizer.from_pretrained(electra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_food_d = pd.read_excel('data/FOOD_DATA.xlsx')\n",
    "r_food_d = r_food_d[r_food_d['SPEAKER']=='고객']\n",
    "r_food_d['TOKENIZED_SENTENCE'] = r_food_d['SENTENCE'].apply(lambda x: tokenizer.tokenize(x))\n",
    "r_food_d['ENCODED_SENTENCE'] = r_food_d['SENTENCE'].apply(lambda x: tokenizer.encode(x))\n",
    "\n",
    "entity_slot = []\n",
    "\n",
    "for vals in r_food_d.values:\n",
    "\n",
    "    # raw_entity = r_food_d.iloc[i]['지식베이스']\n",
    "    raw_entity = vals[-5]\n",
    "    encoded_sentence = vals[-1][1:-1]\n",
    "\n",
    "    if type(raw_entity)==float: # 없는 경우\n",
    "    \n",
    "        entity_slot.append('')\n",
    "        continue\n",
    "\n",
    "    split_entity = raw_entity.split(', ')\n",
    "\n",
    "    raw_entity_label = []\n",
    "    raw_entity_name = []\n",
    "\n",
    "    if len(split_entity)>1: # 여러개 ENTITY\n",
    " \n",
    "        for k in split_entity:      \n",
    "\n",
    "            entity_name = k.split('/')[0]\n",
    "            entity_label = k.split('/')[1]\n",
    "\n",
    "            raw_entity_name.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(entity_name)))\n",
    "            raw_entity_label.append(entity_label)\n",
    "   \n",
    "    else:\n",
    "\n",
    "        split_entity = raw_entity.split('/')\n",
    "\n",
    "        for idx, NAME_or_LABEL in enumerate(split_entity):\n",
    "            \n",
    "            # [이름, 라벨]\n",
    "            if idx % 2 == 0: # ENTITY_NAME\n",
    "\n",
    "                entity_name = NAME_or_LABEL\n",
    "                raw_entity_name.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(entity_name)))\n",
    "\n",
    "            else: # ENTITY_LABEL\n",
    "\n",
    "                entity_label = NAME_or_LABEL\n",
    "                raw_entity_label.append(NAME_or_LABEL) \n",
    "\n",
    "    sentence_token = ', '.join([str(ENCODED_SENT) for ENCODED_SENT in encoded_sentence])\n",
    "   \n",
    "    entity_replace_loc = []\n",
    "    entity_replace_label = []\n",
    "\n",
    "    for n,l in zip(raw_entity_name, raw_entity_label):\n",
    "\n",
    "        token_num_list = [str(token_num) for token_num in n]\n",
    "  \n",
    "        bi_list = ['B-' + l if idx == 0 else 'I-'+ l for idx in range(len(token_num_list))]\n",
    "\n",
    "        joined_t = ', '.join(token_num_list)\n",
    "        joined_bi_list = ', '.join(bi_list)\n",
    "\n",
    "        entity_replace_loc.append(joined_t)\n",
    "        entity_replace_label.append(joined_bi_list)\n",
    "\n",
    "    for loc, label in zip(entity_replace_loc,entity_replace_label):\n",
    "\n",
    "        sentence_token = sentence_token.replace(loc,'#' + label + '#')\n",
    "\n",
    "    entity_result = []\n",
    "\n",
    "    for st_splited in sentence_token.split('#'):\n",
    "\n",
    "        if '-' not in st_splited:\n",
    "\n",
    "            if st_splited == '': continue\n",
    "            \n",
    "            tmp_1 = st_splited.replace(' ','')\n",
    "            tmp_2 = tmp_1.split(',')\n",
    "            tmp_3 = ['O' for x in tmp_2 if x!='']\n",
    "            \n",
    "            entity_result += tmp_3\n",
    "\n",
    "        else:\n",
    "            \n",
    "            entity_result += st_splited.split(', ')\n",
    "            \n",
    "    entity_slot.append(entity_result)\n",
    "\n",
    "r_food_d['ENTITY_SLOT'] = entity_slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_label_list= []\n",
    "\n",
    "for i in r_food_d['ENTITY_SLOT']:\n",
    "    for j in i:\n",
    "        slot_label_list.append(j)\n",
    "\n",
    "slot_label_list = list(set(slot_label_list))\n",
    "\n",
    "append_list = ['PAD','UNK']\n",
    "\n",
    "for tk in append_list:\n",
    "    slot_label_list.append(tk)\n",
    "\n",
    "file_name = './rt_slot_label.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(slot_label_list))  # '\\n' 대신 ', '를 사용하면 줄바꿈이 아닌 ', '를 기준으로 문자열 구분함\n",
    "\n",
    "intent_cat_list = r_food_d['intent_cat'].unique().tolist()\n",
    "intent_cat_list.append('UNK')\n",
    "\n",
    "file_name = './rt_intent_label.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(intent_cat_list))  # '\\n' 대신 ', '를 사용하면 줄바꿈이 아닌 ', '를 기준으로 문자열 구분함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = r_food_d[['SENTENCE','intent_cat','TOKENIZED_SENTENCE','ENCODED_SENTENCE','ENTITY_SLOT']]\n",
    "total_dataset = total_dataset[total_dataset['ENTITY_SLOT']!='']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(total_dataset, train_size= 0.8, random_state=42)\n",
    "\n",
    "train_dataset, dev_dataset = train_test_split(train_dataset, train_size= 0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_in = []\n",
    "\n",
    "for seq_in in train_dataset['SENTENCE']:\n",
    "\n",
    "    train_seq_in.append(seq_in)\n",
    "\n",
    "file_name = './data/train/seq.in.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(train_seq_in)) \n",
    "\n",
    "test_seq_in = []\n",
    "\n",
    "for seq_in in test_dataset['SENTENCE']:\n",
    "\n",
    "    test_seq_in.append(seq_in)\n",
    "\n",
    "file_name = './data/test/seq.in.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(test_seq_in)) \n",
    "\n",
    "dev_seq_in = []\n",
    "\n",
    "for seq_in in dev_dataset['SENTENCE']:\n",
    "\n",
    "    dev_seq_in.append(seq_in)\n",
    "\n",
    "file_name = './data/dev/seq.in.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(dev_seq_in)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "\n",
    "for ic in train_dataset['intent_cat']:\n",
    "\n",
    "    train_label.append(ic)\n",
    "\n",
    "file_name = './data/train/label.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(train_label)) \n",
    "\n",
    "test_label = []\n",
    "\n",
    "for ic in test_dataset['intent_cat']:\n",
    "\n",
    "    test_label.append(ic)\n",
    "\n",
    "file_name = './data/test/label.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(test_label)) \n",
    "\n",
    "dev_label = []\n",
    "\n",
    "for ic in dev_dataset['intent_cat']:\n",
    "\n",
    "    dev_label.append(ic)\n",
    "\n",
    "file_name = './data/dev/label.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(dev_label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_out = []\n",
    "\n",
    "for seq_out_list in train_dataset['ENTITY_SLOT']:\n",
    "\n",
    "    temp_seq_out = []\n",
    "    \n",
    "    for out in seq_out_list:\n",
    "\n",
    "        temp_seq_out.append(out)\n",
    "\n",
    "    temp_seq_out = ' '.join(temp_seq_out)\n",
    "\n",
    "    train_seq_out.append(temp_seq_out)\n",
    "\n",
    "file_name = './data/train/seq.out.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(train_seq_out)) \n",
    "\n",
    "test_seq_out = []\n",
    "\n",
    "for seq_out_list in test_dataset['ENTITY_SLOT']:\n",
    "\n",
    "    temp_seq_out = []\n",
    "\n",
    "    for out in seq_out_list:\n",
    "\n",
    "        temp_seq_out.append(out)\n",
    "\n",
    "    temp_seq_out = ' '.join(temp_seq_out)\n",
    "\n",
    "    test_seq_out.append(temp_seq_out)\n",
    "    \n",
    "\n",
    "file_name = './data/test/seq.out.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(test_seq_out)) \n",
    "\n",
    "dev_seq_out = []\n",
    "\n",
    "for seq_out_list in dev_dataset['ENTITY_SLOT']:\n",
    "\n",
    "    temp_seq_out = []\n",
    "    \n",
    "    for out in seq_out_list:\n",
    "\n",
    "        temp_seq_out.append(out)\n",
    "\n",
    "    temp_seq_out = ' '.join(temp_seq_out)\n",
    "\n",
    "    dev_seq_out.append(temp_seq_out)\n",
    "\n",
    "file_name = './data/dev/seq.out.txt'\n",
    "\n",
    "with open(file_name, 'w+') as file:\n",
    "    file.write('\\n'.join(dev_seq_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = tokenizer.cls_token_id\n",
    "sep_token = tokenizer.sep_token_id\n",
    "unk_token = tokenizer.unk_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "special_tokens_count = 2\n",
    "\n",
    "print(cls_token, sep_token, unk_token, pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraConfig,BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'seed':1234,\n",
    "        'train_batch_size':10,\n",
    "        'eval_batch_size':10,\n",
    "        'max_seq_len':55,\n",
    "        'learning_rate':5e-5,\n",
    "        'num_train_epochs':5,\n",
    "        'weight_decay':0.0,\n",
    "        'gradient_accumulation_steps':1,\n",
    "        'adam_epsilon':1e-8,\n",
    "        'max_grad_norm':1.0,\n",
    "        'max_steps':-1,\n",
    "        'warmup_steps':0,\n",
    "        'dropout_rate':0.1,\n",
    "        'logging_steps':200,\n",
    "        'save_steps':200,\n",
    "        'use_crf': True,\n",
    "        'slot_loss_coef':1.0,\n",
    "        'ignore_index':0,\n",
    "        'model_type':'electra',\n",
    "        'model_name_or_path' : 'monologg/koelectra-base-v3-discriminator',\n",
    "        'pretrained_model_name_or_path' : 'monologg/koelectra-base-v3-discriminator',\n",
    "        'model_dir':'./data/model_save/',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup,ElectraConfig\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "class ElectraPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class JointElectra(ElectraModel):\n",
    "    def __init__(self,electra_path, config, args, intent_label_lst, slot_label_lst):\n",
    "        super(JointElectra,self).__init__(config)\n",
    "        self.args = args\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "        self.electra = ElectraModel(config=self.config)  # Load pretrained electra\n",
    "        self.pooler = ElectraPooler(config)\n",
    "\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args['dropout_rate'])\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args['dropout_rate'])\n",
    "\n",
    "        if args['use_crf']:\n",
    "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_labels_ids):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)  # [CLS]\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        total_loss = 0\n",
    "        # 1. Intent Softmax\n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
    "            else:\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
    "            total_loss += intent_loss\n",
    "\n",
    "        # 2. Slot Softmax\n",
    "        if slot_labels_ids is not None:\n",
    "            if self.args['use_crf']:\n",
    "                slot_loss = self.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
    "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "            else:\n",
    "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args['ignore_index'])\n",
    "                # Only keep active parts of the loss\n",
    "                if attention_mask is not None:\n",
    "                    active_loss = attention_mask.view(-1) == 1\n",
    "                    active_logits = slot_logits.view(-1, self.num_slot_labels)[active_loss]\n",
    "                    active_labels = slot_labels_ids.view(-1)[active_loss]\n",
    "                    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "                else:\n",
    "                    slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_labels_ids.view(-1))\n",
    "            total_loss += self.args['slot_loss_coef'] * slot_loss\n",
    "\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraConfig, ElectraModel, ElectraTokenizer\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'electra': (ElectraConfig, JointElectra, ElectraTokenizer)\n",
    "    }\n",
    "\n",
    "MODEL_PATH_MAP = {\n",
    "    'electra': 'monologg/koelectra-base-v3-discriminator'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_labels(args):\n",
    "    return [label.strip() for label in open(os.path.join('data', 'snips', 'intent_label.txt'), 'r', encoding='utf-8')]\n",
    "\n",
    "\n",
    "def get_slot_labels(args):\n",
    "    return [label.strip() for label in open(os.path.join('data', 'snips', 'slot_label.txt'), 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.intent_label_lst = get_intent_labels(args)\n",
    "        self.slot_label_lst = get_slot_labels(args)\n",
    "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        self.pad_token_label_id = 74\n",
    "        self.config = ElectraConfig.from_pretrained(args['model_name_or_path'], finetuning_task=args.task)\n",
    "        self.model = JointElectra(args['model_name_or_path'],\n",
    "                                #   config=ElectraConfig.from_pretrained(electra_path, finetuning_task='train'),\n",
    "                                  config=self.config,\n",
    "                                  args=args,\n",
    "                                  intent_label_lst=self.intent_label_lst,\n",
    "                                  slot_label_lst=self.slot_label_lst)\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args['train_batch_size'])\n",
    "\n",
    "        if self.args['max_steps'] > 0:\n",
    "            t_total = self.args['max_steps']\n",
    "            self.args['num_train_epochs'] = self.args['max_steps'] // (len(train_dataloader) // self.args['gradient_accumulation_steps']) + 1\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.args['gradient_accumulation_steps'] * self.args['num_train_epochs']\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.args['weight_decay']},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args['learning_rate'], eps=self.args['adam_epsilon'])\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args['warmup_steps'], num_training_steps=t_total)\n",
    "\n",
    "        # Train!\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batchs in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batchs)  # GPU or CPU\n",
    "\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4]}\n",
    "                if self.args['model_type'] != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                if self.args['gradient_accumulation_steps'] > 1:\n",
    "                    loss = loss / self.args['gradient_accumulation_steps']\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.args['gradient_accumulation_steps'] == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args['max_grad_norm'])\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if self.args['logging_steps'] > 0 and global_step % self.args['logging_steps'] == 0:\n",
    "                        self.evaluate(\"dev\")\n",
    "\n",
    "                    if self.args['save_steps'] > 0 and global_step % self.args['save_steps'] == 0:\n",
    "                        self.save_model()\n",
    "\n",
    "                if 0 < self.args['max_steps'] < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if 0 < self.args['max_steps'] < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_file = './data/train/seq.in.txt'\n",
    "intent_label_file = './data/train/label.txt'\n",
    "slot_labels_file = './data/train/seq.out.txt'\n",
    "\n",
    "intent_label_lst = [label.strip() for label in open('./data/rt_intent_label.txt', 'r', encoding='utf-8')]\n",
    "\n",
    "slot_label_lst = [label.strip() for label in open('./data/rt_slot_label.txt','r', encoding='utf-8')]\n",
    "\n",
    "texts = []\n",
    "with open(input_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        texts.append(line.strip())\n",
    "\n",
    "intents = []\n",
    "with open(intent_label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        intents.append(line.strip())\n",
    "\n",
    "slots = []\n",
    "with open(slot_labels_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        slots.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "\n",
    "set_type = 'train'\n",
    "\n",
    "for i, (text, intent, slot) in enumerate(zip(texts, intents, slots)):\n",
    "    \n",
    "    guid = \"%s-%s\" % (set_type, i)\n",
    "    # 1. input_text\n",
    "    words = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))  # Some are spaced twice\n",
    "    # 2. intent\n",
    "    intent_label = intent_label_lst.index(intent) if intent in intent_label_lst else intent_label_lst.index(\"UNK\")\n",
    "    # 3. slot\n",
    "    slot_label = []\n",
    "    for s in slot.split():\n",
    "        slot_label.append(slot_label_lst.index(s) if s in slot_label_lst else slot_label_lst.index(\"UNK\"))\n",
    "        \n",
    "    examples.append(InputExample(guid=guid, words=words, intent_label=intent_label, slot_labels=slot_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(args['model_name_or_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 40\n",
    "pad_token_label_id=slot_label_lst.index(\"PAD\")\n",
    "cls_token_segment_id=0\n",
    "pad_token_segment_id=0\n",
    "sequence_a_segment_id=0\n",
    "mask_padding_with_zero=True\n",
    "\n",
    "cls_token = tokenizer.cls_token_id\n",
    "sep_token = tokenizer.sep_token_id\n",
    "unk_token = tokenizer.unk_token_id\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for (ex_index, example) in enumerate(examples):\n",
    "    \n",
    "    # Tokenize word by word (for NER)\n",
    "    tokens = example.words\n",
    "    slot_labels_ids = example.slot_labels\n",
    "\n",
    "    # Account for [CLS] and [SEP]\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_len - special_tokens_count:\n",
    "        tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "        slot_labels_ids = slot_labels_ids[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # Add [SEP] token\n",
    "    tokens += [sep_token]\n",
    "    slot_labels_ids += [pad_token_label_id]\n",
    "    token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    # Add [CLS] token\n",
    "    tokens = [cls_token] + tokens\n",
    "    slot_labels_ids = [pad_token_label_id] + slot_labels_ids\n",
    "    token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "    input_ids = tokens\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_len - len(input_ids)\n",
    "    input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "    attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "    slot_labels_ids = slot_labels_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
    "    assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "    assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
    "    assert len(slot_labels_ids) == max_seq_len, \"Error with slot labels length {} vs {}\".format(len(slot_labels_ids), max_seq_len)\n",
    "\n",
    "    intent_label_id = int(example.intent_label)\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        intent_label_id=intent_label_id,\n",
    "                        slot_labels_ids=slot_labels_ids\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointElectra(electra_path,\n",
    "                                  config=ElectraConfig.from_pretrained(electra_path, finetuning_task='train'),\n",
    "                                  args=args,\n",
    "                                  intent_label_lst=intent_label_lst,\n",
    "                                  slot_label_lst=slot_label_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "# print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    # Save model checkpoint (Overwrite)\n",
    "    if not os.path.exists(args['model_dir']):\n",
    "        os.makedirs(args['model_dir'])\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(args['model_dir'])\n",
    "\n",
    "    # Save training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args['model_dir'], 'training_args.bin'))\n",
    "    print(\"Saving model checkpoint to %s\",args['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "all_intent_label_ids = torch.tensor([f.intent_label_id for f in features], dtype=torch.long)\n",
    "all_slot_labels_ids = torch.tensor([f.slot_labels_ids for f in features], dtype=torch.long)\n",
    "\n",
    "torch_train_dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
    "                    all_token_type_ids, all_intent_label_ids, all_slot_labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(torch_train_dataset)\n",
    "train_dataloader = DataLoader(torch_train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "\n",
    "if args['max_steps'] > 0:\n",
    "    t_total = args['max_steps']\n",
    "    num_train_epochs = args['max_steps'] // (len(train_dataloader) // args['gradient_accumulation_steps']) + 1\n",
    "else:\n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': args['weight_decay']},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total)\n",
    "\n",
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(texts))\n",
    "logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "logger.info(\"  Total train batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "logger.info(\"  Logging steps = %d\", args['logging_steps'])\n",
    "logger.info(\"  Save steps = %d\", args['save_steps'])\n",
    "\n",
    "global_step = 0\n",
    "tr_loss = 0.0\n",
    "model.zero_grad()\n",
    "\n",
    "train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)  # GPU or CPU\n",
    "\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'intent_label_ids': batch[3],\n",
    "                    'slot_labels_ids': batch[4]}\n",
    "        if args['model_type'] != 'distilbert':\n",
    "            inputs['token_type_ids'] = batch[2]\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if args['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # if logging_steps > 0 and global_step % logging_steps == 0:\n",
    "            #     evaluate(\"dev\")\n",
    "\n",
    "            if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                save_model()\n",
    "\n",
    "        if 0 < args['max_steps'] < global_step:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    if 0 < args['max_steps'] < global_step:\n",
    "        train_iterator.close()\n",
    "        break\n",
    "\n",
    "# global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_file(pred_file_dir):\n",
    "    lines = []\n",
    "    with open(pred_file_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_file_to_tensor_dataset(lines,\n",
    "                                         args,\n",
    "                                         tokenizer,\n",
    "                                         pad_token_label_id=73,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token_id\n",
    "    sep_token = tokenizer.sep_token_id\n",
    "    unk_token = tokenizer.unk_token_id\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    all_slot_label_mask = []\n",
    "    \n",
    "    for words in lines:\n",
    "        print(words)\n",
    "        split_words = tokenizer.tokenize(words)\n",
    "        tokens = tokenizer.convert_tokens_to_ids(split_words)\n",
    "        slot_label_mask = []\n",
    "        slot_label_mask.extend([pad_token_label_id + 1]*len(tokens))\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > args['max_seq_len'] - special_tokens_count:\n",
    "            tokens = tokens[: (args['max_seq_len'] - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(args['max_seq_len'] - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [pad_token_label_id]\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "\n",
    "        input_ids = tokens\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = args['max_seq_len'] - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "        all_slot_label_mask.append(slot_label_mask)\n",
    "\n",
    "    # Change to Tensor\n",
    "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "    all_slot_label_mask = torch.tensor(all_slot_label_mask, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_label_mask)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_text_file = './data/test/seq.in.txt'\n",
    "test_intent_label_file = './data/test/label.txt'\n",
    "test_slot_labels_file = './data/test/seq.out.txt'\n",
    "\n",
    "intent_label_lst = [label.strip() for label in open('./data/rt_intent_label.txt', 'r', encoding='utf-8')]\n",
    "\n",
    "slot_label_lst = [label.strip() for label in open('./data/rt_slot_label.txt', 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_input_file(test_input_text_file)\n",
    "dataset = convert_input_file_to_tensor_dataset(lines, args, tokenizer, pad_token_label_id)\n",
    "\n",
    "# Predict\n",
    "sampler = SequentialSampler(dataset)\n",
    "data_loader = DataLoader(dataset, sampler=sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "all_slot_label_mask = None\n",
    "intent_preds = None\n",
    "slot_preds = None\n",
    "\n",
    "for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {\"input_ids\": batch[0],\n",
    "                    \"attention_mask\": batch[1],\n",
    "                    \"intent_label_ids\": None,\n",
    "                    \"slot_labels_ids\": None}\n",
    "        if args['model_type'] != \"distilbert\":\n",
    "            inputs[\"token_type_ids\"] = batch[2]\n",
    "        outputs = model(**inputs)\n",
    "        _, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "        # Intent Prediction\n",
    "        if intent_preds is None:\n",
    "            intent_preds = intent_logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        # Slot prediction\n",
    "        if slot_preds is None:\n",
    "            if args['use_crf']:\n",
    "                # decode() in `torchcrf` returns list with best index directly\n",
    "                slot_preds = np.array(model.crf.decode(slot_logits))\n",
    "            else:\n",
    "                slot_preds = slot_logits.detach().cpu().numpy()\n",
    "            all_slot_label_mask = batch[3].detach().cpu().numpy()\n",
    "        else:\n",
    "            if args['use_crf']:\n",
    "                slot_preds = np.append(slot_preds, np.array(model.crf.decode(slot_logits)), axis=0)\n",
    "            else:\n",
    "                slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "            all_slot_label_mask = np.append(all_slot_label_mask, batch[3].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "intent_preds = np.argmax(intent_preds, axis=1)\n",
    "\n",
    "if not args['use_crf']:\n",
    "    slot_preds = np.argmax(slot_preds, axis=2)\n",
    "\n",
    "slot_label_map = {i: label for i, label in enumerate(slot_label_lst)}\n",
    "slot_preds_list = [[] for _ in range(slot_preds.shape[0])]\n",
    "\n",
    "for i in range(slot_preds.shape[0]):\n",
    "    for j in range(slot_preds.shape[1]):\n",
    "        if all_slot_label_mask[i, j] != pad_token_label_id:\n",
    "            slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to output file\n",
    "with open('./result.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "    for words, slot_preds, intent_pred in zip(lines, slot_preds_list, intent_preds):\n",
    "        line = \"\"\n",
    "        token_words = tokenizer.tokenize(words)\n",
    "        for word, pred in zip(token_words, slot_preds):\n",
    "            if pred == 'O':\n",
    "                line = line + word + \" \"\n",
    "            else:\n",
    "                line = line + \"[{}:{}] \".format(word, pred)\n",
    "        f.write(\"<{}> -> {}\\n\".format(intent_label_lst[intent_pred], line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
