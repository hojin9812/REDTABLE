{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import pymysql,  os, pickle\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None) # Setting With Copy Warning\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "# implicit 라이브러리에서 권장사항입니다.\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "def live_db_conn():\n",
    "    conn = pymysql.connect(host='host', user='user', password='password', autocommit=True, cursorclass=pymysql.cursors.DictCursor, db=\"db\")\n",
    "    return conn\n",
    "\n",
    "def make_dic(something_list):\n",
    "\n",
    "    name2idx = {}\n",
    "    idx2name = {}\n",
    "\n",
    "    for i in range(len(something_list)):\n",
    "\n",
    "        name2idx[something_list[i]] = i\n",
    "        idx2name[i] = something_list[i]\n",
    "\n",
    "    return name2idx, idx2name\n",
    "\n",
    "def load_token(path, x, y):\n",
    "\n",
    "    x_name2idx = pickle.load(open(path + '{}_name2idx.pkl'.format(x), 'rb'))\n",
    "    x_idx2name = pickle.load(open(path + '{}_idx2name.pkl'.format(x), 'rb'))\n",
    "    y_name2idx = pickle.load(open(path + '{}_name2idx.pkl'.format(y), 'rb'))\n",
    "    y_idx2name = pickle.load(open(path + '{}_idx2name.pkl'.format(y), 'rb'))\n",
    "\n",
    "    return x_name2idx, x_idx2name, y_name2idx, y_idx2name\n",
    "\n",
    "def load_cf(path, user_based_model_name, item_based_model_name):\n",
    "\n",
    "    ub_model = pickle.load(open(path + user_based_model_name, 'rb'))\n",
    "    ib_model = pickle.load(open(path + item_based_model_name, 'rb'))\n",
    " \n",
    "    return ub_model, ib_model\n",
    "\n",
    "def tag_and_type(product_id_list):\n",
    "\n",
    "    conn = live_db_conn()\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    if len(product_id_list) > 1:\n",
    "        sql = \"\"\"\n",
    "        SELECT p.store_id, p.id as product_id, group_concat(ptm.id separator ' ') as ptm_ids, group_concat(pt.type separator ' ') as tag_type\n",
    "        FROM product p\n",
    "        INNER JOIN product_tag pt\n",
    "        INNER JOIN product_tag_master ptm\n",
    "        on pt.product_id=p.id\n",
    "        and pt.product_tag_master_id = ptm.id\n",
    "        WHERE p.id in {}\n",
    "        \"\"\".format(tuple(product_id_list))\n",
    "    \n",
    "    else:\n",
    "\n",
    "        sql = \"\"\"\n",
    "        SELECT p.store_id, p.id as product_id, group_concat(ptm.id separator ' ') as ptm_ids, group_concat(pt.type separator ' ') as tag_type\n",
    "        FROM product p\n",
    "        INNER JOIN product_tag pt\n",
    "        INNER JOIN product_tag_master ptm\n",
    "        on pt.product_id=p.id\n",
    "        and pt.product_tag_master_id = ptm.id\n",
    "        WHERE p.id = {}\n",
    "        \"\"\".format(product_id_list[0])\n",
    "\n",
    "    curs.execute(sql)\n",
    "    df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def rec_product_tag(product_list):\n",
    "\n",
    "    conn = live_db_conn()\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    if len(product_list) > 1:\n",
    "        sql = \"\"\"\n",
    "        SELECT p.store_id as store_id, p.id as product_id, group_concat(ptm.id separator ' ') as ptm_ids, group_concat(pt.type separator ' ') as tag_type\n",
    "        FROM product p\n",
    "        INNER JOIN product_tag pt\n",
    "        INNER JOIN product_tag_master ptm\n",
    "        on pt.product_id=p.id\n",
    "        and pt.product_tag_master_id = ptm.id\n",
    "        WHERE p.id in {} and pt.type != 'option'\n",
    "        group by p.id\n",
    "        \"\"\".format(tuple(product_list))\n",
    "\n",
    "    else:\n",
    "        sql = \"\"\"\n",
    "        SELECT p.store_id as store_id, p.id as product_id, group_concat(ptm.id separator ' ') as ptm_ids, group_concat(pt.type separator ' ') as tag_type\n",
    "        FROM product p\n",
    "        INNER JOIN product_tag pt\n",
    "        INNER JOIN product_tag_master ptm\n",
    "        on pt.product_id=p.id\n",
    "        and pt.product_tag_master_id = ptm.id\n",
    "        WHERE p.id = {} and pt.type != 'option'\n",
    "        group by p.id\n",
    "        \"\"\".format(product_list[0])\n",
    "\n",
    "    curs.execute(sql)\n",
    "    df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_sim_df(input_df, rec_df):    \n",
    "\n",
    "    vectorizer = CountVectorizer(min_df = 1)\n",
    "\n",
    "    full_text = input_df['ptm_ids'].tolist() + rec_df['ptm_ids'].tolist()\n",
    "    X = vectorizer.fit_transform(full_text)\n",
    "    index_list = input_df['product_id'].tolist() + rec_df['product_id'].tolist()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "                            data= X.todense(),\n",
    "                            index = index_list,\n",
    "                            columns = vectorizer.get_feature_names_out()\n",
    "                        )\n",
    "\n",
    "    input_product = input_df['product_id'][0]\n",
    "\n",
    "    input_vector = df[df.index==input_product]\n",
    "\n",
    "    sim_value = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        v = np.linalg.norm(input_vector-df.iloc[i])\n",
    "        sim_value.append(round(v,2))\n",
    "  \n",
    "    df['CB 유사도'] =  sim_value\n",
    "\n",
    "    result = df[['CB 유사도']]\n",
    "    result = result.reset_index().rename(columns={'index':'product_id'})\n",
    "    result.sort_values(by='CB 유사도',ascending=True, inplace=True)\n",
    "    result.reset_index(inplace=True)\n",
    "    result.drop('index',axis=1,inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def type_weight(ptm, t_type):\n",
    "\n",
    "    weight_list = []\n",
    "    weighted_dic = {'ingredient': 7, 'sauce':5, 'cooking':4, 'option':3}\n",
    "    \n",
    "    for i in range(len(ptm)):\n",
    "        for j in range(weighted_dic[t_type[i]]):\n",
    "            weight_list.append(ptm[i])\n",
    "\n",
    "    return weight_list\n",
    "\n",
    "def precision_k(gt, predict, k=10):\n",
    "\n",
    "    intersection = list(set(gt) & set(predict[:k]))\n",
    "\n",
    "    return len(intersection) / k\n",
    "\n",
    "def rel_k(gt, predict, k=10):\n",
    "\n",
    "    return 1 if predict[k-1] in gt else 0\n",
    "\n",
    "def evaluate_map(gt, predict, k=10):\n",
    "    \n",
    "    ap = 0\n",
    "    for i in range(1, min(k, len(gt)) + 1):\n",
    "        \n",
    "        ap += precision_k(gt, predict, i) * rel_k(gt, predict, i)\n",
    "\n",
    "    return ap / min(k, len(gt))\n",
    "\n",
    "def evaluate_recall(gt, predict, k=10):\n",
    "\n",
    "    intersection = list(set(gt) & set(predict))\n",
    "\n",
    "    return min(len(intersection) / min(k, len(gt)),1.0)\n",
    "\n",
    "def product_tag_set(x):\n",
    "\n",
    "    key_list = []\n",
    "    label_count_dic = Counter(x)\n",
    "\n",
    "    for k,v in label_count_dic.items():\n",
    "\n",
    "        if v > 1:\n",
    "\n",
    "            key_list.append(k)\n",
    "\n",
    "    conn = live_db_conn()\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT product_id, group_concat(product_tag_master_id separator ',') as tag_set  FROM product_tag\n",
    "    WHERE product_id in {} and type != 'option'\n",
    "    group by product_id\n",
    "    \"\"\".format(tuple(x))\n",
    "    curs.execute(sql)\n",
    "\n",
    "    df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "    \n",
    "    df['tag_set'] = df['tag_set'].apply(lambda x:set(x.split(',')))\n",
    "    \n",
    "    for key in key_list:\n",
    "        ki = label_count_dic[key]-1\n",
    "        for i in range(label_count_dic[key]-1):\n",
    "            if len(df[df['product_id'] == key]) != 0:\n",
    "                tmp = pd.DataFrame([df[df['product_id'] == key].iloc[0]])\n",
    "                df = pd.concat([df, tmp],axis=0)\n",
    "\n",
    "    tag_set_list = [df[df['product_id'] == i]['tag_set'].values[0] for i in x]\n",
    "    tag_set_list = list(map(tuple, tag_set_list))\n",
    "    \n",
    "    return tag_set_list\n",
    "\n",
    "def tag_based_precision_k(gt, predict, k=10):\n",
    "    # [1,1,2,3] [1,2,3,4]\n",
    "    count = 0 \n",
    "    for pred in predict[:k]:\n",
    "\n",
    "        if pred in set(gt):\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    return count / k\n",
    "\n",
    "def tag_based_rel_k(gt, predict, k=10):\n",
    "\n",
    "    return 1 if predict[k-1] in gt else 0\n",
    "\n",
    "def tag_based_evaluate_map(gt, predict, k=10):\n",
    "    \n",
    "    ap = 0\n",
    "    for i in range(1, min(k, len(gt)) + 1):\n",
    "        \n",
    "        ap += tag_based_precision_k(gt, predict, i) * tag_based_rel_k(gt, predict, i)\n",
    "\n",
    "    return ap / min(k, len(gt))\n",
    "\n",
    "def tag_based_evaluate_recall(gt, predict, k=10):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for pred in predict:\n",
    "\n",
    "        if pred in set(gt):\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    return min(count / min(k, len(gt)),1.0)\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    conn = live_db_conn()\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT o.id, o.user_id as reviewer_name, o.store_id as live_store_id, o.status, op.product_id, op.status as reviewer_stars, op.updated_at, v.reserved_at\n",
    "    FROM `order` o INNER JOIN order_product op on o.id = op.order_id\n",
    "                INNER JOIN product p on op.product_id = p.id\n",
    "                INNER JOIN voucher v on o.id = v.order_id\n",
    "    where p.status = 'normal' and p.sale_status = 'sale'\n",
    "    \"\"\"\n",
    "    curs.execute(sql)\n",
    "\n",
    "    df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "    status_dic = {'normal':5,'cancel':1}\n",
    "\n",
    "    df['reviewer_stars'] = df['reviewer_stars'].apply(lambda x:status_dic[x])\n",
    "\n",
    "    df = df[['id','live_store_id','product_id','reviewer_name','reviewer_stars', 'updated_at', 'reserved_at']]\n",
    "    df.dropna(subset='updated_at', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Loading & Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 개수: 55002\n",
      "유저 수: 11321\n",
      "매장 수: 431\n",
      "상품 수: 4576\n"
     ]
    }
   ],
   "source": [
    "total_review = load_data()\n",
    "print('리뷰 개수:',len(total_review))\n",
    "print('유저 수:',len(total_review['reviewer_name'].unique().tolist()))\n",
    "print('매장 수:',len(total_review['live_store_id'].unique().tolist()))\n",
    "print('상품 수:',len(total_review['product_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학습 데이터 정제: tag_type이 존재하지 않는, 학습에 적절하지 않은 상품들에 대해 학습 데이터에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_in_review = total_review['product_id'].unique().tolist()\n",
    "\n",
    "pir_tag_type = rec_product_tag(product_in_review)\n",
    "\n",
    "pir_tag_type.dropna(subset='tag_type', inplace=True)\n",
    "\n",
    "final_product = pir_tag_type['product_id'].tolist()\n",
    "\n",
    "total_review = total_review[total_review['product_id'].isin(final_product)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_review.sort_values(by=['id','reviewer_name','product_id','updated_at'], ascending=[True,False,False,False], inplace=True)\n",
    "total_review = total_review.drop_duplicates(subset=['id','reviewer_name','product_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_review = total_review.groupby(['live_store_id','product_id','reviewer_name','reserved_at'])['reviewer_stars'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 학습 / 테스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 개수: 13771\n",
      "테스트 데이터 개수: 880\n"
     ]
    }
   ],
   "source": [
    "user_Count = total_review['reviewer_name'].value_counts().reset_index()\n",
    "\n",
    "test_target_review = total_review[total_review['reviewer_name'].isin(user_Count[user_Count['count'] >= 13]['reviewer_name'].tolist())]\n",
    "test_target_review.reset_index(inplace=True)\n",
    "\n",
    "test_review = test_target_review.sort_values(by=['reviewer_name','reserved_at'], ascending=[False,False]).groupby('reviewer_name').head(10)\n",
    "add_train_review = test_target_review[~test_target_review['index'].isin(test_review['index'].tolist())]\n",
    "train_review = total_review[~total_review['reviewer_name'].isin(test_target_review['reviewer_name'].unique().tolist())]\n",
    "\n",
    "user_Count_2 = train_review['reviewer_name'].value_counts().reset_index()\n",
    "\n",
    "final_train_review = train_review[train_review['reviewer_name'].isin(user_Count_2[user_Count_2['count'] >= 3]['reviewer_name'].tolist())]\n",
    "\n",
    "train = pd.concat([final_train_review, add_train_review], axis=0)\n",
    "test = test_review\n",
    "\n",
    "print('학습 데이터 개수:', len(train))\n",
    "print('테스트 데이터 개수:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reviewer-Store Dictionary 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_list = train['reviewer_name'].unique().tolist()\n",
    "product_list = train['product_id'].unique().tolist()\n",
    "\n",
    "reviewer_name2idx, reviewer_idx2name = make_dic(reviewer_list)\n",
    "product_name2idx, product_idx2name = make_dic(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "pickle.dump(reviewer_name2idx, open('../PICKLE/reviewer_name2idx.pkl', 'wb'))\n",
    "pickle.dump(reviewer_idx2name, open('../PICKLE/reviewer_idx2name.pkl', 'wb'))\n",
    "pickle.dump(product_name2idx, open('../PICKLE/product_name2idx.pkl', 'wb'))\n",
    "pickle.dump(product_idx2name, open('../PICKLE/product_idx2name.pkl', 'wb'))\n",
    "\n",
    "# 불러오기\n",
    "reviewer_name2idx = pickle.load(open('../PICKLE/reviewer_name2idx.pkl', 'rb'))\n",
    "reviewer_idx2name = pickle.load(open('../PICKLE/reviewer_idx2name.pkl', 'rb'))\n",
    "product_name2idx = pickle.load(open('../PICKLE/product_name2idx.pkl', 'rb'))\n",
    "product_idx2name = pickle.load(open('../PICKLE/product_idx2name.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collaborative Filtering - Alternating Least Squares 모델 학습을 위한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['reviewer_name'] = train['reviewer_name'].apply(lambda x:reviewer_name2idx[x])\n",
    "train['product_id'] = train['product_id'].apply(lambda x:product_name2idx[x])\n",
    "\n",
    "train[['product_id', 'reviewer_name', 'reviewer_stars']] = train[['product_id', 'reviewer_name', 'reviewer_stars']].astype(float)\n",
    "train[['product_id', 'reviewer_name', 'reviewer_stars']] = train[['product_id', 'reviewer_name', 'reviewer_stars']].astype(int)\n",
    "train.reset_index(inplace=True)\n",
    "\n",
    "train.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviewer = train['reviewer_name'].nunique()\n",
    "num_product = train['product_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. User-Based Als_Model 학습 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3feab2522e4be18fa37c035617a1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviewer_product = csr_matrix((train['reviewer_stars'].values, (train.reviewer_name, train.product_id)), shape= (num_reviewer, num_product))\n",
    "\n",
    "# Implicit Alternating Least Squares 모델 선언\n",
    "ub_als = AlternatingLeastSquares(factors=256, regularization=0.01, use_gpu=False, iterations=10, dtype=np.float32)\n",
    "\n",
    "# 모델 학습\n",
    "ub_als.fit(reviewer_product)\n",
    "\n",
    "# 모델 저장\n",
    "pickle.dump(ub_als, open('../MODEL/ALS_USER_BASED', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Item-Based Als_Model 학습 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7729bee1e3e4c778609a00846e7ed7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "product_reviewer = csr_matrix((train['reviewer_stars'].values, (train.product_id, train.reviewer_name)), shape= (num_product, num_reviewer))\n",
    "\n",
    "# Implicit Alternating Least Squares 모델 선언\n",
    "ib_als = AlternatingLeastSquares(factors=256, regularization=0.01, use_gpu=False, iterations=10, dtype=np.float32)\n",
    "\n",
    "# 모델 훈련\n",
    "ib_als.fit(product_reviewer)\n",
    "\n",
    "# 모델 저장\n",
    "pickle.dump(ib_als, open('../MODEL/ALS_ITEM_BASED', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['reviewer_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:06<00:00, 13.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# 필요 딕셔너리, 모델 \n",
    "reviewer_name2idx, reviewer_idx2name, product_name2idx, product_idx2name = load_token(path='../PICKLE/', x='reviewer', y='product')\n",
    "als_ub, als_ib = load_cf(path='../MODEL/', user_based_model_name='ALS_USER_BASED', item_based_model_name='ALS_ITEM_BASED')\n",
    "\n",
    "# 성능 확인을 위한 리스트 \n",
    "result_list = []\n",
    "\n",
    "for reviewer_name in tqdm(test_id):\n",
    "\n",
    "    reviewer_idx = reviewer_name2idx[reviewer_name]\n",
    "    \n",
    "    # User-Based Alternating Least Squares Model => User Clustering \n",
    "    similar_users = als_ub.similar_users(reviewer_idx, len(als_ub.user_factors))[0].tolist()\n",
    "    \n",
    "    # 비슷한 유저가 구매한 상품이(이후 추천 후보 상품) 10개 이상이 되는 최소 유저의 수를 역으로 계산\n",
    "    product_unique = []\n",
    "    store_unique = []\n",
    "\n",
    "    for idx, user_idx in enumerate(similar_users):\n",
    "\n",
    "        target_df = train[train['reviewer_name'] == user_idx]\n",
    "        target_product_list = target_df['product_id'].unique().tolist()\n",
    "        target_store_list = target_df['live_store_id'].unique().tolist()\n",
    "        product_unique += target_product_list\n",
    "        store_unique += target_store_list\n",
    "        if len(set(store_unique)) >= 10: break\n",
    "    \n",
    "    # 추천 후보 상품의 TAG\n",
    "    target_user_list = similar_users[:idx+1]\n",
    "    target_df = train[train['reviewer_name'].isin(target_user_list)]\n",
    "    target_product_list = target_df['product_id'].unique().tolist()\n",
    "    target_product_list = [product_idx2name[i] for i in target_product_list]\n",
    "    target_tag_df = rec_product_tag(target_product_list)\n",
    "\n",
    "    # 추천 타겟 유저가 구매한 상품들의 TAG\n",
    "    input_df = train[train['reviewer_name']==reviewer_idx]\n",
    "    input_product_list = input_df['product_id'].unique().tolist()\n",
    "    input_product_list = [product_idx2name[i] for i in input_product_list]\n",
    "    input_tag_df = tag_and_type(input_product_list)\n",
    "    input_tag_df['store_id'] = 0\n",
    "    input_tag_df['product_id'] = 0\n",
    "\n",
    "    # 추천 타겟 유저가 구매한 상품이 없거나, 그 상품들의 태그가 없거나, 추천 후보 상품이 없는 예외 케이스\n",
    "    if len(input_tag_df) == 0 or input_tag_df.iloc[0]['ptm_ids'] == None or len(target_tag_df) == 0: continue\n",
    "\n",
    "    # 추천 타겟 유저가 구매한 상품들의 TAG TYPE 가중치 반영\n",
    "    input_ptm = input_tag_df.iloc[0]['ptm_ids'].split(' ')\n",
    "    input_type = input_tag_df.iloc[0]['tag_type'].split(' ')\n",
    "\n",
    "    input_type_weight_list = type_weight(input_ptm, input_type)\n",
    "\n",
    "    input_tag_df.at[0,'ptm_ids'] = ' '.join(input_type_weight_list)\n",
    "\n",
    "    # 추천 후보 상품의 TAG TYPE 가중치 반영\n",
    "    for k in range(len(target_tag_df)):\n",
    "        target_ptm = target_tag_df.iloc[k]['ptm_ids'].split(' ')\n",
    "        target_type = target_tag_df.iloc[k]['tag_type'].split(' ')\n",
    "\n",
    "        target_type_weight_list = type_weight(target_ptm, target_type)\n",
    "        \n",
    "        target_tag_df.at[k,'ptm_ids'] = ' '.join(target_type_weight_list)\n",
    "\n",
    "    # 추천 타겟 유저 구매 상품과 추천 후보 상품의 유사도 계산 => 낮을 수록 비슷한 상품  \n",
    "    result = get_sim_df(input_tag_df, target_tag_df)\n",
    "    \n",
    "    product_store_dic = {}\n",
    "    \n",
    "    target_df['product_id'] = target_df['product_id'].apply(lambda x:product_idx2name[x])\n",
    "    for vals in target_df.values:\n",
    "        product_store_dic[vals[2]] = vals[1]\n",
    "\n",
    "    result = result[result['product_id'] != 0]\n",
    "    result['store_id'] = result['product_id'].apply(lambda x:product_store_dic[x])\n",
    "\n",
    "    # result.drop_duplicates(subset='store_id', inplace=True)\n",
    "    result.sort_values(by='CB 유사도', ascending=True, inplace=True)\n",
    "    \n",
    "    pred = result.head(10)['product_id'].values.tolist()\n",
    "    label = test[test['reviewer_name']==reviewer_name]['product_id'].values.tolist()\n",
    "    result_list.append([reviewer_idx,label,pred])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 10\n",
      "mAP@10: 0.25702065295815296\n",
      "Recall@10: 0.36250000000000004\n"
     ]
    }
   ],
   "source": [
    "final_result = pd.DataFrame(result_list,columns=['user_id','label','pred'])\n",
    "\n",
    "\n",
    "final_result['PRODUCT_BASED:mAP@10'] = final_result.apply(lambda x: evaluate_map(x['label'],x['pred'],10), axis = 1)\n",
    "final_result['PRODUCT_BASED:Recall@10'] = final_result.apply(lambda x: evaluate_recall(x['label'],x['pred'],10), axis = 1)\n",
    "\n",
    "\n",
    "final_result['label_tag_set'] = final_result['label'].apply(lambda x:product_tag_set(x))\n",
    "final_result['pred_tag_set'] = final_result['pred'].apply(lambda x:product_tag_set(x))\n",
    "\n",
    "final_result['TAG_BASED:mAP@10'] = final_result.apply(lambda x: tag_based_evaluate_map(x['label_tag_set'],x['pred_tag_set'],10), axis = 1)\n",
    "final_result['TAG_BASED:Recall@10'] = final_result.apply(lambda x: tag_based_evaluate_recall(x['label_tag_set'],x['pred_tag_set'],10), axis = 1)\n",
    "\n",
    "product_based_result = [\n",
    "    ['10',final_result['PRODUCT_BASED:mAP@10'].mean(),final_result['PRODUCT_BASED:Recall@10'].mean()]\n",
    "                 ]\n",
    "tag_based_result = [\n",
    "    ['10',final_result['TAG_BASED:mAP@10'].mean(),final_result['TAG_BASED:Recall@10'].mean()]\n",
    "                 ]\n",
    "\n",
    "Performace_List = tag_based_result[0]\n",
    "\n",
    "k = Performace_List[0]\n",
    "mAP = Performace_List[1]\n",
    "Recall = Performace_List[2]\n",
    "\n",
    "print('k: {}'.format(k))\n",
    "print('mAP@10: {}'.format(mAP))\n",
    "print('Recall@10: {}'.format(Recall)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_check(user_id):\n",
    "    \n",
    "    label_list = final_result[final_result['user_id'] == user_id]['label'].tolist()[0]\n",
    "    pred_list = final_result[final_result['user_id'] == user_id]['pred'].tolist()[0]\n",
    "\n",
    "    conn = live_db_conn()\n",
    "    curs = conn.cursor()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT id, name from product where id in {}\n",
    "    \"\"\".format(tuple(label_list))\n",
    "    curs.execute(sql)\n",
    "\n",
    "    label_df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT id, name from product where id in {}\n",
    "    \"\"\".format(tuple(pred_list))\n",
    "    curs.execute(sql)\n",
    "\n",
    "    pred_df = pd.DataFrame(curs.fetchall())\n",
    "\n",
    "    curs.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    label_name = label_df['name'].unique().tolist()\n",
    "    pred_name = pred_df['name'].unique().tolist()\n",
    "\n",
    "    print('[유저가 먹은 음식]')\n",
    "\n",
    "    for i in label_name:\n",
    "        print(i)\n",
    "\n",
    "    print('\\n')\n",
    "    print('[추천 받은 음식]')\n",
    "    for j in pred_name:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유저가 먹은 음식]\n",
      "수육(大)\n",
      "간장 갈비찜(大)\n",
      "나주곰탕\n",
      "갈비탕\n",
      "나주육전\n",
      "\n",
      "\n",
      "[추천 받은 음식]\n",
      "한우나주곰탕\n",
      "한우 불고기쌈밥\n",
      "돼지 불고기쌈밥\n",
      "매운 갈비찜(中)\n",
      "나주곰탕\n",
      "나주곰탕(특)\n",
      "갈비탕\n",
      "나주육전\n",
      "특곰탕\n"
     ]
    }
   ],
   "source": [
    "sample_check(2090)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
